{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fbc13b-fa37-40a1-9202-eb228c61cc3f",
   "metadata": {},
   "source": [
    "# LoopTree Tutorial\n",
    "\n",
    "LoopTree is a model to evaluate the latency and energy of a fused-layer dataflow accelerator.\n",
    "\n",
    "To model energy and latency, a workload, architecture, and mapping have to be specified. First, we discuss how these are specified. Then, we show how to run the LoopTree model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3ab36-56ff-471e-8c8a-9a70ac85ee97",
   "metadata": {},
   "source": [
    "## Specifying Architecture, Workload, and Mapping\n",
    "\n",
    "For the LoopTree model to estimate energy and latency, the user must specify an architecture, workload, and mapping. Below, we discuss how to specify each of these inputs.\n",
    "\n",
    "### Architecture\n",
    "In LoopTree, the architecture of an accelerator is abstracted as buffers that use the Buffet semantics and computation units, following the [Timeloop v4 specification](https://timeloop.csail.mit.edu/v4).\n",
    "\n",
    "An example architecture that we will use here is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6e2cf7-29eb-443d-885a-8c0262ca5edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables:\n",
      "  global_cycle_seconds: 1e-9\n",
      "  technology: \"45nm\"\n",
      "\n",
      "architecture:\n",
      "  version: 0.4\n",
      "  nodes:\n",
      "  - !Component\n",
      "    name: MainMemory\n",
      "    class: DRAM\n",
      "    attributes: {width: 256, block_size: 32, word_bits: 8, datawidth: 8}\n",
      "    required_actions: ['read', 'write']\n",
      "  - !Component\n",
      "    name: GlobalBuffer\n",
      "    class: SRAM\n",
      "    attributes:\n",
      "        depth: 8192\n",
      "        width: 256\n",
      "        block_size: 32\n",
      "        word_bits: 8\n",
      "        datawidth: 8\n",
      "        n_rdwr_ports: 2\n",
      "        n_rd_ports: 0\n",
      "        n_wr_ports: 0\n",
      "    required_actions: ['read', 'write']\n",
      "  - !Component\n",
      "    name: MACC\n",
      "    class: intmac\n",
      "    attributes:\n",
      "        datawidth: 8\n",
      "        width: 8\n",
      "        cycle_time: 1e-9\n",
      "    required_actions: ['compute']\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from pprint import pp\n",
    "show_config('architecture.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9392be-34e6-4d8e-9d96-9e5b3c0bc0c2",
   "metadata": {},
   "source": [
    "### Workloads\n",
    "\n",
    "DNN workloads in LoopTree are abstracted as a set of Einsums, each representing a layer.\n",
    "\n",
    "Each Einsum is specified as a dictionary. Each Einsum reads/writes to a number of tensors (referred to as `data_spaces` in the specification), and each tensor is either an input to the operation or an output (indicated by a key \"read_write\" with value `True`). Moreover, a \"projection\" from the Einsum to each tensor, which describes the index of the element in the data that an operation in the Einsum accesses, must be specified. Finally, we specify must specify the *shape* of the Einsum, which is the bounds of its dimensions (also referred to as \"ranks\").\n",
    "\n",
    "An example comprising two fully-connected layers are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab4d488-7b82-4a9d-8f15-1d3debe11aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem:\n",
      "  - shape:\n",
      "      name: Fc1\n",
      "      dimensions: [ P1, M1, C1 ]\n",
      "      data_spaces:\n",
      "      - name: Fmap1\n",
      "        dimensions: [ Fmap1_C, Fmap1_P ]\n",
      "        projection: '[ C1, P1 ]'\n",
      "      - name: Filter1\n",
      "        dimensions: [ Filter1_C, Filter1_M ]\n",
      "        projection: '[ C1, M1 ]'\n",
      "      - name: Fmap2\n",
      "        dimensions: [ Fmap2_C, Fmap2_P ]\n",
      "        projection: '[ M1, P1 ]'\n",
      "        read_write: True\n",
      "\n",
      "    instance: >-\n",
      "      0 <= P1 < 128 and 0 <= M1 < 64 and 0 <= C1 < 64\n",
      "\n",
      "  - shape:\n",
      "      name: Fc2\n",
      "      dimensions: [ P2, M2, C2 ]\n",
      "      data_spaces:\n",
      "      - name: Fmap2\n",
      "        dimensions: [ Fmap2_C, Fmap2_P ]\n",
      "        projection: '[ C2, P2 ]'\n",
      "      - name: Filter2\n",
      "        dimensions: [ Filter2_C, Filter2_M ]\n",
      "        projection: '[ C2, M2 ]'\n",
      "      - name: Fmap3\n",
      "        dimensions: [ Fmap3_C, Fmap3_P ]\n",
      "        projection: '[ M2, P2 ]'\n",
      "        read_write: True\n",
      "\n",
      "    instance: >-\n",
      "      0 <= P2 < 128 and 0 <= M2 < 64 and 0 <= C2 < 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_config('two_fc.workload.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ea13d-6158-44a4-867c-530c52a1cd6a",
   "metadata": {},
   "source": [
    "In this workload specification, we specify two Einsums Fc1 and Fc2. Each Einsum has three dimensions. Fc1 has dimensions P1, M1, and C1; Fc2 has dimensions P2, M2, and C2. Finally, we specify the shape of the Einsums as bounds for each Einsum dimension (rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb0c8b9-59e2-4bb4-a6af-83f617b43c69",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "The LoopTree mapping is a tree-structure that contains nodes of the types described below.\n",
    "- **Loops**: a loop node specifies a rank in the Einsum that is partitioned and the shape of the tiles that results from the partitioning. If the loop is a \"temporal\" loop, then the tiles are scheduled to be processed one at a time. If the loop is a \"spatial\" loop, then the tiles are scheduled to parallel hardware units.\n",
    "- **Branching point**: nodes above a branching point (*i.e.*, ancestor nodes) describe inter-Einsum mapping, which is applied to all Einsums under that branching point. Nodes underneath the branching point (*i.e.*, within each branch) pertain only to the Einsum of that branch.\n",
    "- **Storage**: a storage node specifies the tiles of tensors to retain and which hardware level retains the tiles.\n",
    "- **Compute**: a compute node specifies the hardware level used to compute operations of an Einsum. This node has to be a leaf node, and it also denotes the Einsum that a particular branch pertains to.\n",
    "\n",
    "First, we discuss a layer-by-layer mapping example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd28a74-5330-4624-a414-e5ca545984ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:\n",
      "  type: fused\n",
      "  nodes:\n",
      "  - type: storage  #---------------------------------- node 1.a\n",
      "    target: 0  # level 2 is bound to MainMemory\n",
      "    dspace: [Filter1, Filter2, Fmap1, Fmap2, Fmap3]\n",
      "  - type: sequential  #------------------------------- node 1.b\n",
      "    branches:\n",
      "    - - type: storage #------------------------------- node 1.c\n",
      "        target: 1  # level 1 is bound to GlobalBuffer\n",
      "        dspace: [Filter1]\n",
      "      - type: temporal #------------------------------ node 1.d\n",
      "        rank: P1\n",
      "        tile_shape: 1\n",
      "      - type: storage #------------------------------- node 1.e\n",
      "        target: 1\n",
      "        dspace: [Fmap1, Fmap2]\n",
      "      - type: temporal\n",
      "        rank: C1\n",
      "        tile_shape: 1\n",
      "      - type: temporal\n",
      "        rank: M1\n",
      "        tile_shape: 1\n",
      "      - type: compute #------------------------------- node 1.f\n",
      "        einsum: Fc1\n",
      "        target: 2  # level 0 is bound to MACC\n",
      "    - - type: storage #------------------------------- node 1.g\n",
      "        target: 1\n",
      "        dspace: [Filter2]\n",
      "      - type: temporal\n",
      "        rank: P2\n",
      "        tile_shape: 1\n",
      "      - type: storage\n",
      "        target: 1\n",
      "        dspace: [Fmap2, Fmap3]\n",
      "      - type: temporal\n",
      "        rank: C2\n",
      "        tile_shape: 1\n",
      "      - type: temporal\n",
      "        rank: M2\n",
      "        tile_shape: 1\n",
      "      - type: compute\n",
      "        einsum: Fc2\n",
      "        target: 2  # level 0 is bound to MACC\n"
     ]
    }
   ],
   "source": [
    "show_config('layer-by-layer.mapping.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146686f-b807-491b-af7c-82bfe83bcf53",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- (1.a) The storage node specifies that the tensors specified by the `dspace` key will be retained in target 0, which we will bind to MainMemory.\n",
    "        Note that MainMemory retains Fmap2.\n",
    "- (1.b) The sequential node specifies that the following branches (one for Fc1 and one for Fc2, as we will see shortly) are processed sequentially.\n",
    "- (1.c) Filter1 is retained in GlobalBuffer\n",
    "- (1.d) The rank P1 (which is a rank of the Fc1 Einsum) is partitioned into tiles with shape 1 and we will iterate over the tiles one at a time (temporal iteration).\n",
    "- (1.e) Tiles of Fmap1 and Fmap2 are retained in GlobalBuffer. We retain tiles becuase the P1 rank has been partitioned.\n",
    "- (1.f) Operations of Fc1 are processed in the MACC unit. Moreover, this node specifies that this branch is relevant to Fc2.\n",
    "- (1.g) This and the following nodes specify how Fc2 is mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3eaa59e-8b83-49ef-8f36-755135d6d5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:\n",
      "  type: fused\n",
      "  nodes:\n",
      "  - type: storage\n",
      "    target: 0  # level 2 is bound to MainMemory\n",
      "    dspace: [Filter1, Filter2, Fmap1, Fmap3] #-------- node 2.a\n",
      "  - type: storage\n",
      "    target: 1  # level 1 is bound to GlobalBuffer\n",
      "    dspace: [Filter1, Filter2]\n",
      "  - type: temporal #---------------------------------- node 2.b\n",
      "    rank: P2\n",
      "    tile_shape: 1\n",
      "  - type: storage  #---------------------------------- node 2.c\n",
      "    target: 1  # level 1 is bound to GlobalBuffer\n",
      "    dspace: [Fmap1, Fmap2, Fmap3]\n",
      "  - type: sequential  #------------------------------- node 2.d\n",
      "    branches:\n",
      "    - - type: temporal\n",
      "        rank: C1\n",
      "        tile_shape: 1\n",
      "      - type: temporal\n",
      "        rank: M1\n",
      "        tile_shape: 1\n",
      "      - type: compute\n",
      "        einsum: Fc1\n",
      "        target: 2  # level 0 is bound to MACC\n",
      "    - - type: temporal\n",
      "        rank: C2\n",
      "        tile_shape: 1\n",
      "      - type: temporal\n",
      "        rank: M2\n",
      "        tile_shape: 1\n",
      "      - type: compute\n",
      "        einsum: Fc2\n",
      "        target: 2  # level 0 is bound to MACC\n"
     ]
    }
   ],
   "source": [
    "show_config('fused.mapping.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213bff1-447f-4a80-922c-cef4c638d2eb",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- (2.a) Similar to (1.a), but note that Fmap2 is no longer retained in MainMemory.\n",
    "- (2.b) In this mapping, the $P2$ rank is partitioned to create tiles that are rows of feature maps Fmap1, Fmap2, and Fmap3. The tile shape attribute of the loop node implies that the tiles of Fmap3 have shape 1 in the $P2$ rank. LoopTree infers the shape of the tiles of Fmap2 and Fmap1 based on what is required as inputs to compute the specified tile of Fmap3. In this case, tiles that each contains one row of Fmap2 is required to compute tiles of Fmap3, and tiles that each contains one row of Fmap1 is required to compute tiles of Fmap2 in turn.\n",
    "- (2.c) The tiles of Fmap1, Fmap2, and Fmap3 are retained in GlobalBuffer.\n",
    "- (2.d) Similar to before, Fc1 and Fc2 are processed sequentially. However, as we have put node (2.b) above this sequential node, *tiles* of Fc1 and Fc2 are processed sequentially in alternating fashion: a tile of Fc1 produces a tile of Fmap2, the tile of Fmap2 is consumed by a tile of Fc2, another tile of Fc1 is produced, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc558a8-360c-42ab-a62b-80edd1745924",
   "metadata": {},
   "source": [
    "## Running the Model\n",
    "\n",
    "Using the PyTimeloop library, running the model is as simple as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87522bd4-a683-46ae-884f-b35f01a557e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytimeloop.looptree.run import run_looptree\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'GlobalBuffer',\n",
    "    2: 'MACC'\n",
    "}\n",
    "\n",
    "latency, energy = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\n",
    "    TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf9e3f-e58a-4167-a1d7-499ec6fab091",
   "metadata": {},
   "source": [
    "The result returned by the model contains several information, such as latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b6f2040-068f-4505-95aa-cf183f9632a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 1048576\n"
     ]
    }
   ],
   "source": [
    "print('Latency:', latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95b0b1-e942-4362-ab76-818742ac2a78",
   "metadata": {},
   "source": [
    "and also energy, which is computed by calculating the number of actions to each hardware component and multiplying that with the energy per action estimated using the Accelergy tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4427d1fc-aedf-4ee6-8030-5809a2c92596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy:\n",
      "{('MainMemory', 'read'): 83886080.0,\n",
      " ('GlobalBuffer', 'read'): 516016570.36800003,\n",
      " ('MainMemory', 'write'): 33554432.0,\n",
      " ('GlobalBuffer', 'write'): 164768251.904,\n",
      " ('MACC', 'compute'): 886046.72}\n"
     ]
    }
   ],
   "source": [
    "print('Energy:')\n",
    "pp(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cbe63c-494f-48fc-b341-317ebb2123d0",
   "metadata": {},
   "source": [
    "Now, we compare these results with the fused mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e339a0d-e596-4145-aa2a-4a77faf7aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 1048576\n",
      "Energy:\n",
      "{('MainMemory', 'read'): 50331648.0,\n",
      " ('GlobalBuffer', 'read'): 516016570.36800003,\n",
      " ('MainMemory', 'write'): 16777216.0,\n",
      " ('GlobalBuffer', 'write'): 164768251.904,\n",
      " ('MACC', 'compute'): 886046.72}\n"
     ]
    }
   ],
   "source": [
    "from pytimeloop.looptree.run import run_looptree\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'GlobalBuffer',\n",
    "    2: 'MACC'\n",
    "}\n",
    "\n",
    "latency, energy = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    ['architecture.yaml', 'two_fc.workload.yaml', 'fused.mapping.yaml'],\n",
    "    TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")\n",
    "print('Latency:', latency)\n",
    "print('Energy:')\n",
    "pp(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e41bc-8a1e-4f57-a95f-c8bdd275731d",
   "metadata": {},
   "source": [
    "As we can see, the energy consumption due to MainMemory reads and writes have decreased significantly from fusion.\n",
    "\n",
    "Here, we have not modeled the impact of limited MainMemory bandwidth; thus, because the MACC unit utilization is the same (100%) in both cases, the latency is the same. However, if the layer-by-layer latency is limited by MainMemory bandwidth, fusion will also decrease latency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
